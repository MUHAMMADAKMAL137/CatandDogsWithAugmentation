{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "q0n5fdXtu3Yk"
      },
      "outputs": [],
      "source": [
        "#install kaggle\n",
        "!pip install -q kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "the Kaggle Python package is installed using pip, with the -q flag to suppressthe Kaggle Python package is installed using pip, with the -q flag to suppress"
      ],
      "metadata": {
        "id": "TUgCf6XF3ORc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ju71gGS_FxoM"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The files.upload() function from google.colab is used to prompt the user to upload files to the Google Colab environment. It allows users to select and upload files from their local machine to be used in the Colab notebook."
      ],
      "metadata": {
        "id": "OzabJJHb3VAj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hmxMeSU1Gwky"
      },
      "outputs": [],
      "source": [
        "# Create a Kaggle Directory\n",
        "! mkdir ~/.kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i9mvzvV4HO1I"
      },
      "outputs": [],
      "source": [
        "! cp kaggle.json ~/.kaggle/\n",
        "# copy the kaggle.jasn to created folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WOWY2OgvJmfm"
      },
      "outputs": [],
      "source": [
        "! cp kaggle.json /content/KaggleDataSets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iRbyBSKdKD5N"
      },
      "outputs": [],
      "source": [
        "# to grant premission to jasn to act\n",
        "! chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ehX8v5uuLp9E",
        "outputId": "95906f2f-523b-4179-9fb3-43bb7f6ce07e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ref                                                             title                                                size  lastUpdated          downloadCount  voteCount  usabilityRating  \n",
            "--------------------------------------------------------------  --------------------------------------------------  -----  -------------------  -------------  ---------  ---------------  \n",
            "alphiree/cardiovascular-diseases-risk-prediction-dataset        Cardiovascular Diseases Risk Prediction Dataset       5MB  2023-07-03 12:12:19           4582        192  1.0              \n",
            "nelgiriyewithana/countries-of-the-world-2023                    Global Country Information Dataset 2023              23KB  2023-07-08 20:37:33           3372        127  1.0              \n",
            "arnavsmayan/netflix-userbase-dataset                            Netflix Userbase Dataset                             25KB  2023-07-04 07:38:41           4591         92  1.0              \n",
            "byomokeshsenapati/spotify-song-attributes                       Spotify Song Attributes                             883KB  2023-07-09 16:00:20            715         29  1.0              \n",
            "sumangoda/food-prices                                           Vital Food Costs: A Five-Nation Analysis 2018-2022    8KB  2023-07-16 19:33:29            493         23  1.0              \n",
            "floatingcoder/tmdb-20000-movies-dataset                         Tmdb 20000 movies dataset 2023                      468KB  2023-07-17 15:44:29            482         23  1.0              \n",
            "iamsouravbanerjee/data-science-salaries-2023                    Latest Data Science Salaries                         67KB  2023-07-22 07:42:40           2256         80  1.0              \n",
            "abhijitdahatonde/swiggy-restuarant-dataset                      Swiggy Restuarants dataset                          251KB  2023-07-17 11:44:18            660         21  1.0              \n",
            "rm1000/fortune-500-companies                                    Fortune 500 Companies                               360KB  2023-07-11 01:35:05            955         33  1.0              \n",
            "harishkumardatalab/housing-price-prediction                     Housing Price Prediction                              5KB  2023-07-07 04:34:24           1292         37  1.0              \n",
            "khushipitroda/stock-market-historical-data-of-top-10-companies  Stock Market: Historical Data of Top 10 Companies   476KB  2023-07-18 10:28:12            463         22  1.0              \n",
            "nathaniellybrand/los-angeles-crime-dataset-2020-present         Los Angeles Crime Dataset (2020 -- Present)          37MB  2023-07-09 16:28:16            457         23  1.0              \n",
            "subhajournal/wine-quality-data-combined                         Wine Quality Data (Combined)                        660KB  2023-07-17 16:51:13            389         25  1.0              \n",
            "ujjwalwadhwa/cars24com-used-cars-dataset                        Cars24.com Used Cars dataset                        131KB  2023-07-16 11:04:03            675         23  1.0              \n",
            "howisusmanali/house-prices-2023-dataset                         House Prices 2023 Dataset                            12MB  2023-07-18 03:07:38            564         23  1.0              \n",
            "sanjanchaudhari/employees-performance-for-hr-analytics          Employeeâ€™s Performance for HR Analytics             205KB  2023-07-20 09:08:10            469         29  0.7647059        \n",
            "khushipitroda/imdb-top-250-tv-shows                             IMDB Top 250 TV Shows                                35KB  2023-07-18 13:08:14            348         21  1.0              \n",
            "swathiunnikrishnan/amazon-consumer-behaviour-dataset            Amazon consumer Behaviour Dataset                    15KB  2023-07-06 07:21:42           1697         49  1.0              \n",
            "uom190346a/sleep-health-and-lifestyle-dataset                   Sleep Health and Lifestyle Dataset                    3KB  2023-05-26 10:24:31           8376        179  1.0              \n",
            "manishkumar7432698/airline-passangers-booking-data              Airline Customer Holiday Booking Dataset              2MB  2023-07-17 20:19:27           1694         48  1.0              \n"
          ]
        }
      ],
      "source": [
        "# to list all datasets available in kaggle\n",
        "! kaggle datasets list"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ! at the beginning indicates that the command is executed in the command line or shell within the Colab environment. The command kaggle datasets list is used to list all available datasets on Kaggle, providing users with information about the datasets available for use or exploration."
      ],
      "metadata": {
        "id": "NnTAcywz3dCs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-dzQnykML8Wc",
        "outputId": "ccc6d500-7bbd-449d-991d-fc3fe17bccfc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading dogs-vs-cats.zip to /content\n",
            "100% 812M/812M [00:30<00:00, 30.3MB/s]\n",
            "100% 812M/812M [00:30<00:00, 28.1MB/s]\n"
          ]
        }
      ],
      "source": [
        "! kaggle competitions download -c dogs-vs-cats"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Kaggle API is used to download the dataset for the \"Dogs vs. Cats\" competition in Google Colab. The command fetches the necessary files for the competition, allowing users to access the data and participate in the challenge."
      ],
      "metadata": {
        "id": "3cKohOPs3gyq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ir7lWjpgOAvF",
        "outputId": "813f3c51-e9e5-44ec-ec30-48c8637398df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  dogs-vs-cats.zip\n",
            "  inflating: sampleSubmission.csv    \n",
            "  inflating: test1.zip               \n",
            "  inflating: train.zip               \n"
          ]
        }
      ],
      "source": [
        "! unzip dogs-vs-cats.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_JFBlBZOJ9K"
      },
      "outputs": [],
      "source": [
        "! unzip test1.zip\n",
        "! unzip train.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OhddW8HHOV6v"
      },
      "outputs": [],
      "source": [
        "import os, shutil"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The os and shutil modules are imported, enabling you to perform file and directory operations, such as copying, moving, or deleting files and folders, in Python."
      ],
      "metadata": {
        "id": "-0GOdkS03k_q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m9LcVfbvXTJ5"
      },
      "outputs": [],
      "source": [
        "original_dataset_dir = '/content/train/'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The variable original_dataset_dir is set to the directory path '/content/train/', which represents the location of the original dataset. This directory is likely used as the source from which data is loaded or processed in subsequent operations."
      ],
      "metadata": {
        "id": "UKj8RMKO3pEy"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46JSk2xdZZ1V"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S1k7LmkXXb11"
      },
      "outputs": [],
      "source": [
        "base_dir = '/content/UntitledFolder/'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The variable base_dir is set to the directory path '/content/UntitledFolder/', which represents the base directory where a new folder or project will be created. This directory can be used as the root location to organize and store files and data related to a specific task or project."
      ],
      "metadata": {
        "id": "N1ORcdDh3s9a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j1NTbeviXj8h"
      },
      "outputs": [],
      "source": [
        "train_dir =os.path.join(base_dir, 'train')\n",
        "os.mkdir(train_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The variable train_dir is created by joining the base_dir and the folder name 'train' using the os.path.join() function. Then, the os.mkdir() function is used to create a new directory named 'train' within the base_dir directory, where training data can be stored or organized."
      ],
      "metadata": {
        "id": "F-NZODq24Frx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "shYnfnKzYZ6v"
      },
      "outputs": [],
      "source": [
        "validation_dir =os.path.join(base_dir, 'validation')\n",
        "os.mkdir(validation_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The variable validation_dir is created by joining the base_dir and the folder name 'validation' using the os.path.join() function. Then, the os.mkdir() function is used to create a new directory named 'train' within the base_dir directory, where training data can be stored or organized."
      ],
      "metadata": {
        "id": "zAj0Ct4W4Iji"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_VWk18ztZpYY"
      },
      "outputs": [],
      "source": [
        "test_dir =os.path.join(base_dir, 'test')\n",
        "os.mkdir(test_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The variable test_dir is created by joining the base_dir and the folder name 'test' using the os.path.join() function. Then, the os.mkdir() function is used to create a new directory named 'train' within the base_dir directory, where training data can be stored or organized."
      ],
      "metadata": {
        "id": "WO27DO0w4NZa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4h3a06zXaRdn"
      },
      "outputs": [],
      "source": [
        "train_cats_dir =os.path.join(train_dir, 'cats')\n",
        "os.mkdir(train_cats_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PFUI32xUajbb"
      },
      "outputs": [],
      "source": [
        "train_dogs_dir = os.path.join(train_dir, 'dogs')\n",
        "os.mkdir(train_dogs_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oAxP5ey5azK9"
      },
      "outputs": [],
      "source": [
        "validation_cats_dir =os.path.join(validation_dir, 'cats')\n",
        "os.mkdir(validation_cats_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQ80nSLObCj2"
      },
      "outputs": [],
      "source": [
        "validation_dogs_dir = os.path.join(validation_dir, 'dogs')\n",
        "os.mkdir(validation_dogs_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZzRY6ODbbTdX"
      },
      "outputs": [],
      "source": [
        "test_cats_dir = os.path.join(test_dir, 'cats')\n",
        "os.mkdir(test_cats_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MnL4r00qbhzk"
      },
      "outputs": [],
      "source": [
        "test_dogs_dir = os.path.join(test_dir, 'dogs')\n",
        "os.mkdir(test_dogs_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hWf6OSF2bzY9"
      },
      "outputs": [],
      "source": [
        "fnames = ['cat.{}.jpg'.format(i) for i in range(1000)]\n",
        "for fname in fnames:\n",
        "  srs= os.path.join(original_dataset_dir, fname)\n",
        "  dst =os.path.join(train_cats_dir, fname)\n",
        "  shutil.copyfile(srs, dst)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Here i am copying 1000 cat images with filenames in the format 'cat.i.jpg', where i ranges from 0 to 999, from the original_dataset_dir to the train_cats_dir."
      ],
      "metadata": {
        "id": "ibK1kenA4Sh-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dg_ZRGnNdHVA"
      },
      "outputs": [],
      "source": [
        "fnames = ['cat.{}.jpg'.format(i) for i in range(1000,1500)]\n",
        "for fname in fnames:\n",
        "  srs = os.path.join(original_dataset_dir, fname)\n",
        "  dst = os.path.join(validation_cats_dir, fname)\n",
        "  shutil.copyfile(srs, dst)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here i copies 500 cat images with filenames in the format 'cat.i.jpg', where i ranges from 1000 to 1499, from the original_dataset_dir to the validation_cats_dir."
      ],
      "metadata": {
        "id": "1m3k8Duc42H6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CLj6IPNYeCvt"
      },
      "outputs": [],
      "source": [
        "fnames = ['cat.{}.jpg'.format(i) for i in range(1500, 2000)]\n",
        "for fname in fnames:\n",
        "  srs = os.path.join(original_dataset_dir, fname)\n",
        "  dst = os.path.join(test_cats_dir, fname)\n",
        "  shutil.copyfile(srs, dst)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here i am copying 500 cat images with filenames in the format 'cat.i.jpg', where i ranges from 1500 to 1999, from the original_dataset_dir to the test_cats_dir"
      ],
      "metadata": {
        "id": "GvH91Q1E5Jxw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yXu5rgZieqH9"
      },
      "outputs": [],
      "source": [
        "fnames = ['dog.{}.jpg'.format(i) for i in range(1000)]\n",
        "for fname in fnames:\n",
        "  src = os.path.join(original_dataset_dir, fname)\n",
        "  dst = os.path.join(train_dogs_dir, fname)\n",
        "  shutil.copyfile(src, dst)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here i am copying 1000 dog images with filenames in the format 'dog.i.jpg', where i ranges from 0 to 999, from the original_dataset_dir to the train_dogs_dir."
      ],
      "metadata": {
        "id": "J4-eTs1B5iSS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "K-cIa2uf5dkJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X5q8mUdUf45z"
      },
      "outputs": [],
      "source": [
        "fnames = ['dog.{}.jpg'.format(i) for i in range(1000, 1500)]\n",
        "for fname in fnames:\n",
        "  srs =os.path.join(original_dataset_dir, fname)\n",
        "  dst = os.path.join(validation_dogs_dir, fname)\n",
        "  shutil.copyfile(srs, dst)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Here i am copying 500 dog images with filenames in the format 'dog.i.jpg', where i ranges from 1000 to 1499, from the original_dataset_dir to the validation_dogs_dir."
      ],
      "metadata": {
        "id": "TqoER7qI6_jK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tfTmWmt6gaSP"
      },
      "outputs": [],
      "source": [
        "fnames = ['dog.{}.jpg'.format(i) for i in range(1500, 2000)]\n",
        "for fname in fnames:\n",
        "  srs = os.path.join(original_dataset_dir, fname)\n",
        "  dst = os.path.join(test_dogs_dir, fname)\n",
        "  shutil.copyfile(srs, dst)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here i am copying 500 dog images with filenames in the format 'dog.i.jpg', where i ranges from 1500 to 1999, from the original_dataset_dir to the test_dogs_dir."
      ],
      "metadata": {
        "id": "an7iESw77Iux"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ZoVpJNEg6Kf",
        "outputId": "44c77377-a8aa-4749-804c-6ee45bd6ed86"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total training cats images 1000\n",
            "total training dogs images 1000\n"
          ]
        }
      ],
      "source": [
        "print('total training cats images', len(os.listdir(train_cats_dir)))\n",
        "print('total training dogs images', len(os.listdir(train_dogs_dir)))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The total number of training images for both cats and dogs classes in the specified directories."
      ],
      "metadata": {
        "id": "PO0XYFfE7UK7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mu2OzQbPhSJF",
        "outputId": "6963ecaf-c93c-401f-eb5f-80c0c5b409e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total validation cats images 500\n",
            "total validation dogs images 500\n"
          ]
        }
      ],
      "source": [
        "print('total validation cats images', len(os.listdir(validation_cats_dir)))\n",
        "print('total validation dogs images', len(os.listdir(validation_dogs_dir)))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The total number of validation images for both cats and dogs classes in the specified directories."
      ],
      "metadata": {
        "id": "Mzcx2S1B7aBr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pO5N1boTjnoe",
        "outputId": "8c2163dc-920f-4781-afc7-ff34d84e1012"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total test cats images 500\n",
            "total test dogs images 500\n"
          ]
        }
      ],
      "source": [
        "print('total test cats images', len(os.listdir(test_cats_dir)))\n",
        "print('total test dogs images', len(os.listdir(test_dogs_dir)))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The total number of test images for both cats and dogs classes in the specified directories."
      ],
      "metadata": {
        "id": "uBh6F2T37di6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "RuY6NYJukQ2u"
      },
      "outputs": [],
      "source": [
        "from keras import models\n",
        "from keras import layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AWe_7LAzoGk7"
      },
      "outputs": [],
      "source": [
        "model =models.Sequential()\n",
        "model.add(layers.Conv2D(32, (3,3), activation = 'relu', input_shape = (150,150,3)))\n",
        "model.add(layers.MaxPooling2D(2,2))\n",
        "model.add(layers.Conv2D(64, (3,3), activation = 'relu'))\n",
        "model.add(layers.MaxPooling2D(2,2))\n",
        "model.add(layers.Conv2D(128,(3,3), activation = 'relu'))\n",
        "model.add(layers.MaxPooling2D(2,2))\n",
        "model.add(layers.Conv2D(128, (3,3), activation = 'relu'))\n",
        "model.add(layers.MaxPooling2D(2,2))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(512, activation = 'relu'))\n",
        "model.add(layers.Dense(1, activation = 'sigmoid'))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here i defines a sequential model in Keras for image classification. It consists of multiple Conv2D and MaxPooling2D layers, followed by a Flatten layer to convert the 2D feature maps to a 1D vector. It then includes two dense layers with ReLU activation and a final dense layer with sigmoid activation for binary classification (sigmoid outputs a probability between 0 and 1 for binary classification)."
      ],
      "metadata": {
        "id": "FdRPbUAi7quO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJEERLXeoaaL",
        "outputId": "9bec8e6b-d0e8-402e-9a22-3316e60e3e34"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 148, 148, 32)      896       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 74, 74, 32)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 72, 72, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 36, 36, 64)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 34, 34, 128)       73856     \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 17, 17, 128)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 15, 15, 128)       147584    \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPooling  (None, 7, 7, 128)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 6272)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 512)               3211776   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 513       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,453,121\n",
            "Trainable params: 3,453,121\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The provided summary describes a neural network model for image classification. It consists of four convolutional layers with max-pooling, followed by a flatten layer to convert the 2D feature maps to a 1D vector. The model then includes two dense layers with ReLU activation, and a final dense layer with sigmoid activation for binary classification. The model has a total of 3,453,121 trainable parameters."
      ],
      "metadata": {
        "id": "R8iUozWN8BFc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here i displayed the summary of the model."
      ],
      "metadata": {
        "id": "ueeP2n6P7t9K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "cl4UZKJRpzRR"
      },
      "outputs": [],
      "source": [
        "from keras import optimizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GXI_WvRqp-wq"
      },
      "outputs": [],
      "source": [
        "model.compile(loss='binary_crossentropy',optimizer=optimizers.RMSprop(learning_rate=1e-4),metrics=['acc'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here i compiles the neural network model using binary cross-entropy as the loss function, RMSprop optimizer with a learning rate of 1e-4, and accuracy as the evaluation metric."
      ],
      "metadata": {
        "id": "yvED9rZA8lDU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8idMispwqfIG"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kHjIKBZYrYSG"
      },
      "outputs": [],
      "source": [
        "train_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "test_datagen = ImageDataGenerator(rescale = 1./255)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code sets up two image data generators, train_datagen and test_datagen, which are used for data preprocessing and augmentation during training and testing. The rescale parameter is set to 1./255, which means the pixel values of the images will be scaled to the range [0, 1]."
      ],
      "metadata": {
        "id": "kysMuetT8yJ-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPJKpqHZrtPu",
        "outputId": "f19f6e7c-f0a2-4f4a-ab5e-9f4c304b848c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 2000 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "train_generator = train_datagen.flow_from_directory(\n",
        "train_dir,\n",
        "target_size=(150, 150),\n",
        "batch_size=20,\n",
        "class_mode='binary')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code creates a train_generator using the flow_from_directory method of the train_datagen. It generates batches of training data from the directory specified by train_dir. The images are resized to a target size of (150, 150), and each batch contains 20 images. The class mode is set to 'binary' because it is a binary classification task (two classes - cats and dogs)."
      ],
      "metadata": {
        "id": "svZLvCET9HBu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xi4Xc8P5sSoa",
        "outputId": "c94c37c0-7538-43cd-c84a-b96357137ba2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 1000 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "validation_generator = test_datagen.flow_from_directory(validation_dir, target_size=(150,150), batch_size=20, class_mode='binary')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code creates a validation_generator using the flow_from_directory method of the test_datagen. It generates batches of validation data from the directory specified by validation_dir. The images are resized to a target size of (150, 150), and each batch contains 20 images. The class mode is set to 'binary' because it is a binary classification task (two classes - cats and dogs) used for validation during model training."
      ],
      "metadata": {
        "id": "zGxD-Smt9UjV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "405z8d7ps9kB",
        "outputId": "e06c1b09-71f6-49c9-935e-cc3bec924794"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data batch shape (20, 150, 150, 3)\n",
            "Data labels Shape (20,)\n"
          ]
        }
      ],
      "source": [
        "for data_batch, labels_batch in train_generator:\n",
        "  print('Data batch shape', data_batch.shape)\n",
        "  print('Data labels Shape', labels_batch.shape)\n",
        "  break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ElInYi3svojU",
        "outputId": "648d269d-8d4b-4dd1-f478-5719fbb790e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data batch shape (20, 150, 150, 3)\n",
            "Data labels Shape (20,)\n"
          ]
        }
      ],
      "source": [
        "for data_batch, labels_batch in validation_generator:\n",
        "  print('Data batch shape', data_batch.shape)\n",
        "  print('Data labels Shape', labels_batch.shape)\n",
        "  break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bUt6nXMS0-bT"
      },
      "outputs": [],
      "source": [
        "history = model.fit_generator(train_generator, steps_per_epoch=100, epochs= 30, validation_data = validation_generator, validation_steps=50 )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code trains the model using the fit_generator function. It uses the train_generator to provide training data and the validation_generator to provide validation data. The model is trained for 30 epochs, and for each epoch, it performs 100 steps using the training data and 50 steps using the validation data. The training progress and evaluation results are stored in the history variable."
      ],
      "metadata": {
        "id": "BxV46o6r9fr7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ujguf8f1yDA"
      },
      "outputs": [],
      "source": [
        "model.save('cats_and_dogs_small_1.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code saves the trained model to a file named 'cats_and_dogs_small_1.h5'. This file will contain the model's architecture, weights, and other relevant information, allowing you to load and reuse the model in the future without having to retrain it."
      ],
      "metadata": {
        "id": "LPKn6r_F9nUj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YCzxXYldJ-Y5"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs = range(1, len(acc) + 1)\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code uses Matplotlib to plot the training and validation accuracy in one figure and the training and validation loss in another figure. It uses blue dots for the training accuracy and loss, and solid blue lines for the validation accuracy and loss. The title and legends are added to the plots for better understanding, and finally, the plots are displayed using plt.show()."
      ],
      "metadata": {
        "id": "0bEj53E49xhT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aCCMkC6AKBlI"
      },
      "outputs": [],
      "source": [
        "datagen = ImageDataGenerator(rotation_range=40, width_shift_range=2.0, height_shift_range=2.0, shear_range=2.0, zoom_range=2.0, horizontal_flip=True, fill_mode='nearest')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code creates an `ImageDataGenerator` object called `datagen`, which will be used for data augmentation. It applies various transformations to the images during training, such as rotation (up to 40 degrees), horizontal and vertical shifting (up to 2.0 pixels), shearing (up to 2.0), zooming (up to 2.0 times), horizontal flipping, and filling any missing pixels after transformations using the 'nearest' method. These augmentations help increase the diversity of the training data and can improve the model's generalization and performance."
      ],
      "metadata": {
        "id": "2L_HUGcj_d8q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BwAAzJ5piq36"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing import image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LVFr7ZJ9i58B"
      },
      "outputs": [],
      "source": [
        "fnames = [os.path.join(train_cats_dir, fname) for\n",
        "          fname in os.listdir(train_cats_dir)]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code creates a list called fnames, which contains the file paths of all the image files in the train_cats_dir directory. It uses a list comprehension to iterate through the files in the directory and joins the file name with the directory path using os.path.join() function. This list will be used for further processing, such as feeding the image data into the model during training."
      ],
      "metadata": {
        "id": "-0MQNwD1_qzD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LVIaPI4snBOK"
      },
      "outputs": [],
      "source": [
        "from keras.layers.serialization import activation\n",
        "model =models.Sequential()\n",
        "model.add(layers.Conv2D(32,(3,3), activation = 'relu', input_shape =(150,150,3)))\n",
        "model.add(layers.MaxPooling2D(2,2))\n",
        "model.add(layers.Conv2D(64, (3,3), activation = 'relu'))\n",
        "model.add(layers.MaxPooling2D(2,2))\n",
        "model.add(layers.Conv2D(128, (3,3), activation = 'relu'))\n",
        "model.add(layers.MaxPooling2D(2,2))\n",
        "model.add(layers.Conv2D(128, (3,3), activation = 'relu'))\n",
        "model.add(layers.MaxPooling2D(2,2))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.Dense(512, activation = 'relu'))\n",
        "model.add(layers.Dense(1, activation = 'sigmoid'))\n",
        "model.compile(loss='binary_crossentropy',optimizer=optimizers.RMSprop(learning_rate=1e-4),metrics=['acc'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code defines a deep learning model using Keras. The model consists of several layers, including Conv2D layers with ReLU activation, MaxPooling2D layers, a Flatten layer, a Dropout layer with dropout rate of 0.5, and two Dense layers with ReLU and sigmoid activations, respectively. The model is designed for binary classification tasks, where the input images have a shape of (150, 150, 3). In last we compiled it."
      ],
      "metadata": {
        "id": "oXOIXSa7_7si"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HOifgUwSoTQn",
        "outputId": "ee0c834e-77c5-433b-b5dd-26bbc4621495"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_4 (Conv2D)           (None, 148, 148, 32)      896       \n",
            "                                                                 \n",
            " max_pooling2d_4 (MaxPooling  (None, 74, 74, 32)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 72, 72, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d_5 (MaxPooling  (None, 36, 36, 64)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_6 (Conv2D)           (None, 34, 34, 128)       73856     \n",
            "                                                                 \n",
            " max_pooling2d_6 (MaxPooling  (None, 17, 17, 128)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_7 (Conv2D)           (None, 15, 15, 128)       147584    \n",
            "                                                                 \n",
            " max_pooling2d_7 (MaxPooling  (None, 7, 7, 128)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 6272)              0         \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 6272)              0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 512)               3211776   \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1)                 513       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,453,121\n",
            "Trainable params: 3,453,121\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eGiV7wpMogyP"
      },
      "outputs": [],
      "source": [
        "train_datagen = ImageDataGenerator(\n",
        "    rescale = 1./255,\n",
        "    rotation_range = 40,\n",
        "    width_shift_range=2.0,\n",
        "    height_shift_range= 2.0,\n",
        "    shear_range= 2.0,\n",
        "    zoom_range=2.0,\n",
        "    horizontal_flip=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code initializes an ImageDataGenerator for training data with various data augmentation techniques, including rescaling the pixel values to the range [0, 1], random rotation up to 40 degrees, random shifts in width and height up to 2.0 times the original size, shear transformation up to 2.0, zooming in and out up to 2.0, and horizontal flipping. This data generator is useful for creating augmented batches of training images, which helps improve the model's generalization and robustness."
      ],
      "metadata": {
        "id": "Yb0V7ODkAgUT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-5NviUT0zeAW"
      },
      "outputs": [],
      "source": [
        "test_datagen = ImageDataGenerator(rescale= 1./255)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Educd_RRz5PE",
        "outputId": "4e9c95a3-c278-460f-95e4-79d634822e56"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 2000 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "train_data_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=20,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code sets up a data generator `train_data_generator` using the previously defined `train_datagen`. It will generate batches of training data from the `train_dir` directory, where the images are resized to (150, 150) pixels, the batch size is set to 20, and the class mode is set to binary (since it's a binary classification problem - cats vs. dogs). This generator will be used for training the model using the `fit_generator` method."
      ],
      "metadata": {
        "id": "hoe5vHPaAqLV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-GPMvej05lO",
        "outputId": "274708a0-6ea6-4e42-e44e-f087db5396a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 1000 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "validation_generator = test_datagen.flow_from_directory(validation_dir, target_size=(150,150), batch_size=20, class_mode ='binary')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code sets up another data generator `validation_generator` using the `test_datagen`. It will generate batches of validation data from the `validation_dir` directory, where the images are resized to (150, 150) pixels, the batch size is set to 20, and the class mode is set to binary (since it's a binary classification problem - cats vs. dogs). This generator will be used for validating the model's performance during training using the `fit_generator` method."
      ],
      "metadata": {
        "id": "f8XskB5wAyMj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3DURvZ-jlM9c"
      },
      "outputs": [],
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "checkpoint = ModelCheckpoint('model_check_point.h5', monitor = 'val_loss', save_best_only=True, save_weights_only= False, verbose= 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code imports the `ModelCheckpoint` callback from Keras. It creates an instance of `ModelCheckpoint` with the following arguments:\n",
        "- `'model_check_point.h5'`: The filename to save the model.\n",
        "- `monitor='val_loss'`: The metric to monitor for saving the best model, which is the validation loss in this case.\n",
        "- `save_best_only=True`: It indicates to only save the model when the validation loss improves from the previous best.\n",
        "- `save_weights_only=False`: It indicates to save the entire model (including architecture and optimizer state) rather than just the weights.\n",
        "- `verbose=1`: It specifies the verbosity level, setting it to 1 will show a progress bar when saving the model."
      ],
      "metadata": {
        "id": "HKBkTjDjA6zs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1uOYSVPK1rB8"
      },
      "outputs": [],
      "source": [
        "history = model.fit_generator(\n",
        "train_generator,\n",
        "steps_per_epoch=100,\n",
        "epochs=20,\n",
        "validation_data=validation_generator,\n",
        "validation_steps=50,callbacks =[checkpoint] )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xka6dtuz2wRE"
      },
      "outputs": [],
      "source": [
        "model.save('cats_and_dogs_small_2.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "y0ECS1lm2Tzq"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Md8TTi382fi3"
      },
      "outputs": [],
      "source": [
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs = range(1, len(acc)+1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ziFEUglB3Kps"
      },
      "outputs": [],
      "source": [
        "plt.plot(epochs, acc, 'bo', label = 'training accuracy' )\n",
        "plt.plot(epochs, val_acc, 'b', label = 'validation_accuracy')\n",
        "plt.title('training and validation accuracy with augmentation')\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, 'bo', label ='training_loss')\n",
        "plt.plot(epochs, val_loss, 'b', label = 'validation_loss')\n",
        "plt.title('training and validation loss with augmentation')\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "JVm-99jS4idG"
      },
      "outputs": [],
      "source": [
        "from keras.applications import VGG16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qPo394fxqYjU",
        "outputId": "590ec329-5834-4a9f-d775-e87554d6f21a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58889256/58889256 [==============================] - 3s 0us/step\n"
          ]
        }
      ],
      "source": [
        "conve_base =VGG16(weights='imagenet', include_top=False, input_shape= (150, 150, 3))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code imports the VGG16 model with pre-trained weights from the 'imagenet' dataset. It sets `include_top` to False, which means the top (classification) layers of the VGG16 model are not included. It also specifies the input shape of the model to be (150, 150, 3) for 3-channel color images of size 150x150 pixels. This configuration allows the VGG16 model to be used as a feature extractor for a custom classification task."
      ],
      "metadata": {
        "id": "SAi9kNZEBTlu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uTMAGTZ7sNAi",
        "outputId": "0df94593-a051-40bc-a2e5-2186ef10594a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<bound method Model.summary of <keras.engine.functional.Functional object at 0x7b1b97141e10>>"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conve_base.summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "67PkwwkwsSI5"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "h25Mzf7Aam8z"
      },
      "outputs": [],
      "source": [
        "# Extracting features using the pretrained convolutional base\n",
        "from keras.preprocessing.image import ImageDataGenerator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqGAnlt2a8fr"
      },
      "outputs": [],
      "source": [
        "base_dir = '/content/UntitledFolder'\n",
        "train_dir =os.path.join(base_dir, 'train')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZFeVWPqbkfc"
      },
      "outputs": [],
      "source": [
        "validation_dir = os.path.join(base_dir, 'validation')\n",
        "test_dir = os.path.join(base_dir, 'test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e6V27n4ubzeu"
      },
      "outputs": [],
      "source": [
        "datagen = ImageDataGenerator(rescale=1./255)\n",
        "batch_size = 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5EAPVwPJcSkd"
      },
      "outputs": [],
      "source": [
        "\n",
        "def extract_features(directory, sample_count):\n",
        "\n",
        "  features = np.zeros(shape= (sample_count, 4, 4, 512))\n",
        "  labels = np.zeros(shape=(sample_count))\n",
        "  generator = datagen.flow_from_directory(\n",
        "      directory,\n",
        "      target_size= (150 , 150),\n",
        "      batch_size = batch_size,\n",
        "      class_mode = 'binary'\n",
        "    )\n",
        "  i = 0\n",
        "  for inputs_batch, labels_batch in generator:\n",
        "    features_batch = conve_base.predict(inputs_batch)\n",
        "    features[i * batch_size : (i + 1) * batch_size] = features_batch\n",
        "    labels[ i * batch_size : (i + 1) * batch_size] = labels_batch\n",
        "    i += 1\n",
        "    if i * batch_size>= sample_count:\n",
        "      break\n",
        "    return features, labels\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The given code defines a function extract_features that takes a directory and the total number of samples as input. It initializes empty arrays features and labels to store the extracted features and corresponding labels. The function uses the datagen generator to retrieve batches of data and their labels from the specified directory. It then passes each batch of data through the pre-trained VGG16 model (conve_base) to extract features, and saves the features and labels in the respective arrays. The function returns the extracted features and labels."
      ],
      "metadata": {
        "id": "l3t69T4KClNG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FOoiGq2-c3A2",
        "outputId": "512633f4-e2f9-487b-cbae-da30fe746fbf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 2000 images belonging to 2 classes.\n",
            "1/1 [==============================] - 7s 7s/step\n"
          ]
        }
      ],
      "source": [
        "train_features, train_labels = extract_features(train_dir, 2000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "guuvuW4IhufR",
        "outputId": "81231556-918d-46bc-cf9c-89d6b798d552"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 1000 images belonging to 2 classes.\n",
            "1/1 [==============================] - 5s 5s/step\n"
          ]
        }
      ],
      "source": [
        "validation_features, validation_labels = extract_features(validation_dir, 1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r8_MnaArjrNZ",
        "outputId": "ca6a0806-6723-4814-fd22-d1193bbbf61e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 1000 images belonging to 2 classes.\n",
            "1/1 [==============================] - 5s 5s/step\n"
          ]
        }
      ],
      "source": [
        "test_features, test_labels = extract_features(test_dir, 1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W5TdfpSMj6CN",
        "outputId": "75fb9794-1630-4009-c569-ea3fab00b321"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(2000, 4, 4, 512)"
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_features.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U4UOPsnckCyu",
        "outputId": "c993e59a-ef2d-415f-eb36-991f2ab0127e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(2000, 8192)"
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_features = np.reshape(train_features, (2000, 4*4*512))\n",
        "train_features.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xPFqGczCkjEr",
        "outputId": "1e79b304-3f09-4195-ab30-ca69cc9d7eac"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1000, 8192)"
            ]
          },
          "execution_count": 71,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "validation_features = np.reshape(validation_features, (1000, 4*4*512))\n",
        "validation_features.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jfMDi06fk7uS",
        "outputId": "bb1c7a00-69cf-4e18-b1e5-30b14bea5a11"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1000, 8192)"
            ]
          },
          "execution_count": 72,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_features =np.reshape(test_features, (1000, 4*4*512))\n",
        "test_features.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "BokbFpt5lEdw"
      },
      "outputs": [],
      "source": [
        "model = models.Sequential()\n",
        "model.add(layers.Dense(256, activation = 'relu', input_dim =4*4*512 ))\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.Dense(1, activation ='sigmoid'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code defines a sequential model in Keras. It starts with a dense layer having 256 units and ReLU activation function. The input dimension is set to `4*4*512`, which corresponds to the flattened output shape of the VGG16 model's last layer. A dropout layer with a rate of 0.5 is added to prevent overfitting. Finally, it ends with a dense layer with 1 unit and a sigmoid activation function for binary classification. This model is likely to be used in conjunction with the extracted features from the VGG16 model for binary classification tasks."
      ],
      "metadata": {
        "id": "2K9CoBgIDBg_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y8O0eWBZnbYl"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer = optimizers.RMSprop(learning_rate=2e-5), loss = 'binary_crossentropy', metrics =['acc'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9BrveHY6s6O8"
      },
      "outputs": [],
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "check_point = ModelCheckpoint('dogs-vs-cats-with-features-extration.h5',monitor='val_loss', save_best_only=True, save_weights_only=False, verbose =1 )\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code imports the `ModelCheckpoint` callback from Keras, which allows you to save the model's weights or entire model during training. It creates a callback object named `check_point` with the following settings:\n",
        "- `'dogs-vs-cats-with-features-extration.h5'`: The filename to save the model.\n",
        "- `monitor='val_loss'`: It monitors the validation loss during training.\n",
        "- `save_best_only=True`: It saves the model only if the validation loss has improved.\n",
        "- `save_weights_only=False`: It saves the entire model (not just weights).\n",
        "- `verbose=1`: It displays progress updates during training."
      ],
      "metadata": {
        "id": "UrW3efBADJ6W"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fhuVeTGatKMC",
        "outputId": "2e9be79a-40f7-45a5-bdb8-1923e50bacd0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 7.1840e-08 - acc: 1.0000\n",
            "Epoch 1: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 3s 31ms/step - loss: 7.1840e-08 - acc: 1.0000 - val_loss: 0.0146 - val_acc: 0.9980\n",
            "Epoch 2/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 9.2793e-08 - acc: 1.0000\n",
            "Epoch 2: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 3s 30ms/step - loss: 9.2793e-08 - acc: 1.0000 - val_loss: 0.0146 - val_acc: 0.9980\n",
            "Epoch 3/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 7.4479e-08 - acc: 1.0000\n",
            "Epoch 3: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 22ms/step - loss: 7.4162e-08 - acc: 1.0000 - val_loss: 0.0146 - val_acc: 0.9980\n",
            "Epoch 4/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 1.1388e-07 - acc: 1.0000\n",
            "Epoch 4: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 21ms/step - loss: 1.1354e-07 - acc: 1.0000 - val_loss: 0.0146 - val_acc: 0.9980\n",
            "Epoch 5/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 6.9450e-08 - acc: 1.0000\n",
            "Epoch 5: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 22ms/step - loss: 6.9450e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9980\n",
            "Epoch 6/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 6.0173e-08 - acc: 1.0000\n",
            "Epoch 6: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 3s 33ms/step - loss: 6.0173e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9980\n",
            "Epoch 7/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 7.6722e-08 - acc: 1.0000\n",
            "Epoch 7: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 4s 40ms/step - loss: 7.6410e-08 - acc: 1.0000 - val_loss: 0.0146 - val_acc: 0.9980\n",
            "Epoch 8/200\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 6.1287e-08 - acc: 1.0000\n",
            "Epoch 8: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 3s 28ms/step - loss: 6.0652e-08 - acc: 1.0000 - val_loss: 0.0146 - val_acc: 0.9980\n",
            "Epoch 9/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 4.8090e-08 - acc: 1.0000\n",
            "Epoch 9: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 21ms/step - loss: 4.7967e-08 - acc: 1.0000 - val_loss: 0.0146 - val_acc: 0.9980\n",
            "Epoch 10/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 6.5616e-08 - acc: 1.0000\n",
            "Epoch 10: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 22ms/step - loss: 6.5616e-08 - acc: 1.0000 - val_loss: 0.0146 - val_acc: 0.9980\n",
            "Epoch 11/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 5.5006e-08 - acc: 1.0000\n",
            "Epoch 11: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 21ms/step - loss: 5.5006e-08 - acc: 1.0000 - val_loss: 0.0146 - val_acc: 0.9980\n",
            "Epoch 12/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 5.3485e-08 - acc: 1.0000\n",
            "Epoch 12: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 22ms/step - loss: 5.3485e-08 - acc: 1.0000 - val_loss: 0.0146 - val_acc: 0.9980\n",
            "Epoch 13/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 6.1578e-08 - acc: 1.0000\n",
            "Epoch 13: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 3s 30ms/step - loss: 6.1578e-08 - acc: 1.0000 - val_loss: 0.0146 - val_acc: 0.9980\n",
            "Epoch 14/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 5.2437e-08 - acc: 1.0000\n",
            "Epoch 14: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 3s 29ms/step - loss: 5.2203e-08 - acc: 1.0000 - val_loss: 0.0146 - val_acc: 0.9980\n",
            "Epoch 15/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 6.4873e-08 - acc: 1.0000\n",
            "Epoch 15: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 21ms/step - loss: 6.4873e-08 - acc: 1.0000 - val_loss: 0.0146 - val_acc: 0.9980\n",
            "Epoch 16/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 5.1508e-08 - acc: 1.0000\n",
            "Epoch 16: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 22ms/step - loss: 5.2411e-08 - acc: 1.0000 - val_loss: 0.0146 - val_acc: 0.9980\n",
            "Epoch 17/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 5.0690e-08 - acc: 1.0000\n",
            "Epoch 17: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 21ms/step - loss: 5.0690e-08 - acc: 1.0000 - val_loss: 0.0146 - val_acc: 0.9980\n",
            "Epoch 18/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 4.7749e-08 - acc: 1.0000\n",
            "Epoch 18: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 22ms/step - loss: 4.7749e-08 - acc: 1.0000 - val_loss: 0.0146 - val_acc: 0.9980\n",
            "Epoch 19/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 5.4702e-08 - acc: 1.0000\n",
            "Epoch 19: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 3s 32ms/step - loss: 5.4702e-08 - acc: 1.0000 - val_loss: 0.0146 - val_acc: 0.9980\n",
            "Epoch 20/200\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 5.7668e-08 - acc: 1.0000\n",
            "Epoch 20: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 3s 27ms/step - loss: 5.7630e-08 - acc: 1.0000 - val_loss: 0.0146 - val_acc: 0.9980\n",
            "Epoch 21/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 5.6828e-08 - acc: 1.0000\n",
            "Epoch 21: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 21ms/step - loss: 5.6828e-08 - acc: 1.0000 - val_loss: 0.0146 - val_acc: 0.9980\n",
            "Epoch 22/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 4.4328e-08 - acc: 1.0000\n",
            "Epoch 22: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 21ms/step - loss: 4.4328e-08 - acc: 1.0000 - val_loss: 0.0146 - val_acc: 0.9980\n",
            "Epoch 23/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 5.7206e-08 - acc: 1.0000\n",
            "Epoch 23: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 22ms/step - loss: 5.7206e-08 - acc: 1.0000 - val_loss: 0.0146 - val_acc: 0.9980\n",
            "Epoch 24/200\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 8.9177e-08 - acc: 1.0000\n",
            "Epoch 24: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 23ms/step - loss: 8.7709e-08 - acc: 1.0000 - val_loss: 0.0146 - val_acc: 0.9980\n",
            "Epoch 25/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 6.5066e-08 - acc: 1.0000\n",
            "Epoch 25: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 3s 32ms/step - loss: 6.5066e-08 - acc: 1.0000 - val_loss: 0.0146 - val_acc: 0.9980\n",
            "Epoch 26/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 5.4307e-08 - acc: 1.0000\n",
            "Epoch 26: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 3s 27ms/step - loss: 5.4063e-08 - acc: 1.0000 - val_loss: 0.0146 - val_acc: 0.9980\n",
            "Epoch 27/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 3.8441e-08 - acc: 1.0000\n",
            "Epoch 27: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 21ms/step - loss: 3.8204e-08 - acc: 1.0000 - val_loss: 0.0146 - val_acc: 0.9980\n",
            "Epoch 28/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 6.2377e-08 - acc: 1.0000\n",
            "Epoch 28: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 22ms/step - loss: 6.1883e-08 - acc: 1.0000 - val_loss: 0.0146 - val_acc: 0.9980\n",
            "Epoch 29/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 4.4329e-08 - acc: 1.0000\n",
            "Epoch 29: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 21ms/step - loss: 4.4734e-08 - acc: 1.0000 - val_loss: 0.0146 - val_acc: 0.9980\n",
            "Epoch 30/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 7.5731e-08 - acc: 1.0000\n",
            "Epoch 30: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 22ms/step - loss: 7.5731e-08 - acc: 1.0000 - val_loss: 0.0146 - val_acc: 0.9980\n",
            "Epoch 31/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 5.1761e-08 - acc: 1.0000\n",
            "Epoch 31: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 3s 33ms/step - loss: 5.1334e-08 - acc: 1.0000 - val_loss: 0.0146 - val_acc: 0.9980\n",
            "Epoch 32/200\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 4.9512e-08 - acc: 1.0000\n",
            "Epoch 32: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 3s 27ms/step - loss: 4.8912e-08 - acc: 1.0000 - val_loss: 0.0146 - val_acc: 0.9980\n",
            "Epoch 33/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 4.4600e-08 - acc: 1.0000\n",
            "Epoch 33: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 21ms/step - loss: 4.4600e-08 - acc: 1.0000 - val_loss: 0.0146 - val_acc: 0.9980\n",
            "Epoch 34/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 3.7885e-08 - acc: 1.0000\n",
            "Epoch 34: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 22ms/step - loss: 3.7668e-08 - acc: 1.0000 - val_loss: 0.0146 - val_acc: 0.9980\n",
            "Epoch 35/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 3.3207e-08 - acc: 1.0000\n",
            "Epoch 35: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 3s 31ms/step - loss: 3.3207e-08 - acc: 1.0000 - val_loss: 0.0146 - val_acc: 0.9980\n",
            "Epoch 36/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 4.8373e-08 - acc: 1.0000\n",
            "Epoch 36: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 4.8888e-08 - acc: 1.0000 - val_loss: 0.0146 - val_acc: 0.9980\n",
            "Epoch 37/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 7.7801e-08 - acc: 1.0000\n",
            "Epoch 37: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 3s 35ms/step - loss: 7.7204e-08 - acc: 1.0000 - val_loss: 0.0146 - val_acc: 0.9980\n",
            "Epoch 38/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 4.7812e-08 - acc: 1.0000\n",
            "Epoch 38: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 4.7750e-08 - acc: 1.0000 - val_loss: 0.0146 - val_acc: 0.9980\n",
            "Epoch 39/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 4.1420e-08 - acc: 1.0000\n",
            "Epoch 39: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 21ms/step - loss: 4.1420e-08 - acc: 1.0000 - val_loss: 0.0146 - val_acc: 0.9980\n",
            "Epoch 40/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 4.6946e-08 - acc: 1.0000\n",
            "Epoch 40: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 21ms/step - loss: 4.6946e-08 - acc: 1.0000 - val_loss: 0.0146 - val_acc: 0.9980\n",
            "Epoch 41/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 6.4621e-08 - acc: 1.0000\n",
            "Epoch 41: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 22ms/step - loss: 6.4621e-08 - acc: 1.0000 - val_loss: 0.0146 - val_acc: 0.9980\n",
            "Epoch 42/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 3.9386e-08 - acc: 1.0000\n",
            "Epoch 42: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 3s 28ms/step - loss: 3.9386e-08 - acc: 1.0000 - val_loss: 0.0146 - val_acc: 0.9980\n",
            "Epoch 43/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 3.3452e-08 - acc: 1.0000\n",
            "Epoch 43: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 3s 34ms/step - loss: 3.3452e-08 - acc: 1.0000 - val_loss: 0.0146 - val_acc: 0.9980\n",
            "Epoch 44/200\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 3.7098e-08 - acc: 1.0000\n",
            "Epoch 44: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 23ms/step - loss: 3.7132e-08 - acc: 1.0000 - val_loss: 0.0146 - val_acc: 0.9980\n",
            "Epoch 45/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 4.5336e-08 - acc: 1.0000\n",
            "Epoch 45: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 22ms/step - loss: 4.5227e-08 - acc: 1.0000 - val_loss: 0.0146 - val_acc: 0.9980\n",
            "Epoch 46/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 3.6855e-08 - acc: 1.0000\n",
            "Epoch 46: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 23ms/step - loss: 3.6671e-08 - acc: 1.0000 - val_loss: 0.0146 - val_acc: 0.9980\n",
            "Epoch 47/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 2.8925e-08 - acc: 1.0000\n",
            "Epoch 47: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 22ms/step - loss: 2.9075e-08 - acc: 1.0000 - val_loss: 0.0146 - val_acc: 0.9980\n",
            "Epoch 48/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 4.7955e-08 - acc: 1.0000\n",
            "Epoch 48: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 3s 27ms/step - loss: 4.8084e-08 - acc: 1.0000 - val_loss: 0.0146 - val_acc: 0.9980\n",
            "Epoch 49/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 4.2226e-08 - acc: 1.0000\n",
            "Epoch 49: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 3s 34ms/step - loss: 4.6138e-08 - acc: 1.0000 - val_loss: 0.0146 - val_acc: 0.9980\n",
            "Epoch 50/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 4.1765e-08 - acc: 1.0000\n",
            "Epoch 50: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 21ms/step - loss: 4.1765e-08 - acc: 1.0000 - val_loss: 0.0146 - val_acc: 0.9980\n",
            "Epoch 51/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 3.6140e-08 - acc: 1.0000\n",
            "Epoch 51: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 23ms/step - loss: 3.6140e-08 - acc: 1.0000 - val_loss: 0.0146 - val_acc: 0.9980\n",
            "Epoch 52/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 3.2768e-08 - acc: 1.0000\n",
            "Epoch 52: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 21ms/step - loss: 3.2856e-08 - acc: 1.0000 - val_loss: 0.0146 - val_acc: 0.9980\n",
            "Epoch 53/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 3.4555e-08 - acc: 1.0000\n",
            "Epoch 53: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 21ms/step - loss: 3.4311e-08 - acc: 1.0000 - val_loss: 0.0146 - val_acc: 0.9980\n",
            "Epoch 54/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 3.9804e-08 - acc: 1.0000\n",
            "Epoch 54: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 3s 27ms/step - loss: 4.2430e-08 - acc: 1.0000 - val_loss: 0.0146 - val_acc: 0.9980\n",
            "Epoch 55/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 5.3976e-08 - acc: 1.0000\n",
            "Epoch 55: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 3s 34ms/step - loss: 5.3976e-08 - acc: 1.0000 - val_loss: 0.0146 - val_acc: 0.9980\n",
            "Epoch 56/200\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 3.0489e-08 - acc: 1.0000\n",
            "Epoch 56: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 23ms/step - loss: 3.0552e-08 - acc: 1.0000 - val_loss: 0.0146 - val_acc: 0.9980\n",
            "Epoch 57/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 2.8468e-08 - acc: 1.0000\n",
            "Epoch 57: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 23ms/step - loss: 2.8848e-08 - acc: 1.0000 - val_loss: 0.0146 - val_acc: 0.9980\n",
            "Epoch 58/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 3.1158e-08 - acc: 1.0000\n",
            "Epoch 58: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 22ms/step - loss: 3.0949e-08 - acc: 1.0000 - val_loss: 0.0146 - val_acc: 0.9980\n",
            "Epoch 59/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 4.1354e-08 - acc: 1.0000\n",
            "Epoch 59: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 21ms/step - loss: 4.1354e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9980\n",
            "Epoch 60/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 3.4705e-08 - acc: 1.0000\n",
            "Epoch 60: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 3s 31ms/step - loss: 3.4466e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9980\n",
            "Epoch 61/200\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 3.8499e-08 - acc: 1.0000\n",
            "Epoch 61: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 3s 29ms/step - loss: 3.7907e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9980\n",
            "Epoch 62/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 2.9208e-08 - acc: 1.0000\n",
            "Epoch 62: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 21ms/step - loss: 2.9208e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9980\n",
            "Epoch 63/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 3.4860e-08 - acc: 1.0000\n",
            "Epoch 63: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 22ms/step - loss: 3.4860e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9980\n",
            "Epoch 64/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 2.9711e-08 - acc: 1.0000\n",
            "Epoch 64: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 21ms/step - loss: 2.9711e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9980\n",
            "Epoch 65/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 3.5834e-08 - acc: 1.0000\n",
            "Epoch 65: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 21ms/step - loss: 3.5605e-08 - acc: 1.0000 - val_loss: 0.0148 - val_acc: 0.9980\n",
            "Epoch 66/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 2.8130e-08 - acc: 1.0000\n",
            "Epoch 66: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 3s 31ms/step - loss: 2.8130e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9980\n",
            "Epoch 67/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 3.1105e-08 - acc: 1.0000\n",
            "Epoch 67: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 3s 30ms/step - loss: 3.1105e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9980\n",
            "Epoch 68/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 3.7823e-08 - acc: 1.0000\n",
            "Epoch 68: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 23ms/step - loss: 3.7823e-08 - acc: 1.0000 - val_loss: 0.0148 - val_acc: 0.9980\n",
            "Epoch 69/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 3.4256e-08 - acc: 1.0000\n",
            "Epoch 69: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 22ms/step - loss: 3.4256e-08 - acc: 1.0000 - val_loss: 0.0148 - val_acc: 0.9980\n",
            "Epoch 70/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 2.1808e-08 - acc: 1.0000\n",
            "Epoch 70: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 22ms/step - loss: 2.1619e-08 - acc: 1.0000 - val_loss: 0.0148 - val_acc: 0.9980\n",
            "Epoch 71/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 3.0482e-08 - acc: 1.0000\n",
            "Epoch 71: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 21ms/step - loss: 3.0482e-08 - acc: 1.0000 - val_loss: 0.0148 - val_acc: 0.9980\n",
            "Epoch 72/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 2.6562e-08 - acc: 1.0000\n",
            "Epoch 72: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 3s 30ms/step - loss: 2.6562e-08 - acc: 1.0000 - val_loss: 0.0148 - val_acc: 0.9980\n",
            "Epoch 73/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 3.3684e-08 - acc: 1.0000\n",
            "Epoch 73: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 3s 31ms/step - loss: 3.3441e-08 - acc: 1.0000 - val_loss: 0.0148 - val_acc: 0.9980\n",
            "Epoch 74/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 2.7087e-08 - acc: 1.0000\n",
            "Epoch 74: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 21ms/step - loss: 2.8119e-08 - acc: 1.0000 - val_loss: 0.0148 - val_acc: 0.9980\n",
            "Epoch 75/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 3.1283e-08 - acc: 1.0000\n",
            "Epoch 75: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 22ms/step - loss: 3.1068e-08 - acc: 1.0000 - val_loss: 0.0148 - val_acc: 0.9980\n",
            "Epoch 76/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 2.5517e-08 - acc: 1.0000\n",
            "Epoch 76: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 22ms/step - loss: 2.5494e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9980\n",
            "Epoch 77/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 4.5430e-08 - acc: 1.0000\n",
            "Epoch 77: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 22ms/step - loss: 4.5430e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9980\n",
            "Epoch 78/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 2.3750e-08 - acc: 1.0000\n",
            "Epoch 78: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 3s 31ms/step - loss: 2.3750e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9980\n",
            "Epoch 79/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 2.6829e-08 - acc: 1.0000\n",
            "Epoch 79: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 4s 38ms/step - loss: 2.6724e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9980\n",
            "Epoch 80/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 3.2246e-08 - acc: 1.0000\n",
            "Epoch 80: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 3s 35ms/step - loss: 3.2406e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9980\n",
            "Epoch 81/200\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 3.5813e-08 - acc: 1.0000\n",
            "Epoch 81: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 22ms/step - loss: 3.5558e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9980\n",
            "Epoch 82/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 3.0353e-08 - acc: 1.0000\n",
            "Epoch 82: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 22ms/step - loss: 3.0150e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9980\n",
            "Epoch 83/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 3.5104e-08 - acc: 1.0000\n",
            "Epoch 83: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 21ms/step - loss: 3.4867e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9980\n",
            "Epoch 84/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 2.9699e-08 - acc: 1.0000\n",
            "Epoch 84: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 23ms/step - loss: 2.9750e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9980\n",
            "Epoch 85/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 2.7052e-08 - acc: 1.0000\n",
            "Epoch 85: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 3s 33ms/step - loss: 2.6972e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9980\n",
            "Epoch 86/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 2.5516e-08 - acc: 1.0000\n",
            "Epoch 86: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 3s 26ms/step - loss: 2.5516e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9980\n",
            "Epoch 87/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 2.6827e-08 - acc: 1.0000\n",
            "Epoch 87: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 21ms/step - loss: 2.6634e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9980\n",
            "Epoch 88/200\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 2.7955e-08 - acc: 1.0000\n",
            "Epoch 88: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 21ms/step - loss: 2.7798e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9980\n",
            "Epoch 89/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 1.9495e-08 - acc: 1.0000\n",
            "Epoch 89: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 22ms/step - loss: 1.9869e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9980\n",
            "Epoch 90/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 2.3270e-08 - acc: 1.0000\n",
            "Epoch 90: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 23ms/step - loss: 2.3270e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9980\n",
            "Epoch 91/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 7.2138e-08 - acc: 1.0000\n",
            "Epoch 91: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 3s 34ms/step - loss: 7.1468e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9980\n",
            "Epoch 92/200\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 1.7886e-08 - acc: 1.0000\n",
            "Epoch 92: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 3s 27ms/step - loss: 1.8123e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9980\n",
            "Epoch 93/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 1.9813e-08 - acc: 1.0000\n",
            "Epoch 93: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 21ms/step - loss: 1.9813e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9980\n",
            "Epoch 94/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 2.2978e-08 - acc: 1.0000\n",
            "Epoch 94: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 21ms/step - loss: 2.2978e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9980\n",
            "Epoch 95/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 2.2863e-08 - acc: 1.0000\n",
            "Epoch 95: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 22ms/step - loss: 2.2769e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9980\n",
            "Epoch 96/200\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 1.8439e-08 - acc: 1.0000\n",
            "Epoch 96: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 23ms/step - loss: 1.8526e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9980\n",
            "Epoch 97/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 3.0875e-08 - acc: 1.0000\n",
            "Epoch 97: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 3s 34ms/step - loss: 3.0599e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9980\n",
            "Epoch 98/200\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 2.0079e-08 - acc: 1.0000\n",
            "Epoch 98: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 3s 26ms/step - loss: 2.1635e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9980\n",
            "Epoch 99/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 2.3871e-08 - acc: 1.0000\n",
            "Epoch 99: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 22ms/step - loss: 2.3728e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9980\n",
            "Epoch 100/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 3.4600e-08 - acc: 1.0000\n",
            "Epoch 100: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 22ms/step - loss: 3.4600e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9980\n",
            "Epoch 101/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 2.3402e-08 - acc: 1.0000\n",
            "Epoch 101: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 21ms/step - loss: 2.3313e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9980\n",
            "Epoch 102/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 2.3365e-08 - acc: 1.0000\n",
            "Epoch 102: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 23ms/step - loss: 2.3365e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9980\n",
            "Epoch 103/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 2.1511e-08 - acc: 1.0000\n",
            "Epoch 103: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 3s 34ms/step - loss: 2.1511e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9980\n",
            "Epoch 104/200\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 2.0129e-08 - acc: 1.0000\n",
            "Epoch 104: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 3s 25ms/step - loss: 2.0057e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9980\n",
            "Epoch 105/200\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 1.6849e-08 - acc: 1.0000\n",
            "Epoch 105: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 22ms/step - loss: 1.6730e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9980\n",
            "Epoch 106/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 2.7671e-08 - acc: 1.0000\n",
            "Epoch 106: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 22ms/step - loss: 2.7464e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9980\n",
            "Epoch 107/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 2.3018e-08 - acc: 1.0000\n",
            "Epoch 107: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 23ms/step - loss: 2.3018e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9980\n",
            "Epoch 108/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 2.0984e-08 - acc: 1.0000\n",
            "Epoch 108: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 23ms/step - loss: 2.1100e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9980\n",
            "Epoch 109/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 2.3584e-08 - acc: 1.0000\n",
            "Epoch 109: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 3s 34ms/step - loss: 2.7627e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9980\n",
            "Epoch 110/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 2.1159e-08 - acc: 1.0000\n",
            "Epoch 110: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 3s 27ms/step - loss: 2.1159e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9980\n",
            "Epoch 111/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 1.8603e-08 - acc: 1.0000\n",
            "Epoch 111: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 23ms/step - loss: 1.9276e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9980\n",
            "Epoch 112/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 2.0048e-08 - acc: 1.0000\n",
            "Epoch 112: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 21ms/step - loss: 2.0048e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9980\n",
            "Epoch 113/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 1.8497e-08 - acc: 1.0000\n",
            "Epoch 113: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 23ms/step - loss: 1.8497e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9980\n",
            "Epoch 114/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 2.1378e-08 - acc: 1.0000\n",
            "Epoch 114: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 3s 27ms/step - loss: 2.1378e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9980\n",
            "Epoch 115/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 2.1960e-08 - acc: 1.0000\n",
            "Epoch 115: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 3s 30ms/step - loss: 2.1910e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9980\n",
            "Epoch 116/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 3.0816e-08 - acc: 1.0000\n",
            "Epoch 116: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 3s 25ms/step - loss: 3.0596e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9980\n",
            "Epoch 117/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 2.1841e-08 - acc: 1.0000\n",
            "Epoch 117: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 23ms/step - loss: 2.1841e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9980\n",
            "Epoch 118/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 1.6800e-08 - acc: 1.0000\n",
            "Epoch 118: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 23ms/step - loss: 1.6709e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9980\n",
            "Epoch 119/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 1.6978e-08 - acc: 1.0000\n",
            "Epoch 119: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 23ms/step - loss: 1.6978e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9980\n",
            "Epoch 120/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 1.6612e-08 - acc: 1.0000\n",
            "Epoch 120: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 3s 25ms/step - loss: 1.6612e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9980\n",
            "Epoch 121/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 2.1192e-08 - acc: 1.0000\n",
            "Epoch 121: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 3s 31ms/step - loss: 2.1149e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9980\n",
            "Epoch 122/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 2.2630e-08 - acc: 1.0000\n",
            "Epoch 122: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 3s 26ms/step - loss: 2.2495e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9980\n",
            "Epoch 123/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 2.0478e-08 - acc: 1.0000\n",
            "Epoch 123: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 22ms/step - loss: 2.0403e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9980\n",
            "Epoch 124/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 2.2383e-08 - acc: 1.0000\n",
            "Epoch 124: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 22ms/step - loss: 2.2383e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9980\n",
            "Epoch 125/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 1.4023e-08 - acc: 1.0000\n",
            "Epoch 125: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 23ms/step - loss: 1.3928e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9980\n",
            "Epoch 126/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 2.4089e-08 - acc: 1.0000\n",
            "Epoch 126: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 23ms/step - loss: 2.3866e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9980\n",
            "Epoch 127/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 1.3744e-08 - acc: 1.0000\n",
            "Epoch 127: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 3s 34ms/step - loss: 1.3660e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9980\n",
            "Epoch 128/200\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 1.9957e-08 - acc: 1.0000\n",
            "Epoch 128: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 3s 26ms/step - loss: 1.9710e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9980\n",
            "Epoch 129/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 1.4813e-08 - acc: 1.0000\n",
            "Epoch 129: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 22ms/step - loss: 1.4813e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9980\n",
            "Epoch 130/200\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 2.3903e-08 - acc: 1.0000\n",
            "Epoch 130: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 2.3831e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9980\n",
            "Epoch 131/200\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 1.4113e-08 - acc: 1.0000\n",
            "Epoch 131: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 23ms/step - loss: 1.4041e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9980\n",
            "Epoch 132/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 1.2924e-08 - acc: 1.0000\n",
            "Epoch 132: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 3s 28ms/step - loss: 1.2924e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9980\n",
            "Epoch 133/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 1.5820e-08 - acc: 1.0000\n",
            "Epoch 133: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 3s 34ms/step - loss: 1.5833e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9980\n",
            "Epoch 134/200\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 1.7095e-08 - acc: 1.0000\n",
            "Epoch 134: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 22ms/step - loss: 1.6848e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9980\n",
            "Epoch 135/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 1.5576e-08 - acc: 1.0000\n",
            "Epoch 135: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 22ms/step - loss: 1.5576e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9980\n",
            "Epoch 136/200\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 1.2793e-08 - acc: 1.0000\n",
            "Epoch 136: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 22ms/step - loss: 1.2847e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9980\n",
            "Epoch 137/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 1.4460e-08 - acc: 1.0000\n",
            "Epoch 137: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 23ms/step - loss: 1.4460e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9980\n",
            "Epoch 138/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 1.7094e-08 - acc: 1.0000\n",
            "Epoch 138: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 3s 31ms/step - loss: 1.7095e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9980\n",
            "Epoch 139/200\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 1.7350e-08 - acc: 1.0000\n",
            "Epoch 139: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 3s 30ms/step - loss: 1.7423e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9980\n",
            "Epoch 140/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 1.3761e-08 - acc: 1.0000\n",
            "Epoch 140: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 22ms/step - loss: 1.3761e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9980\n",
            "Epoch 141/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 1.3600e-08 - acc: 1.0000\n",
            "Epoch 141: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 23ms/step - loss: 1.3586e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9980\n",
            "Epoch 142/200\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 1.7037e-08 - acc: 1.0000\n",
            "Epoch 142: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 22ms/step - loss: 1.6935e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9980\n",
            "Epoch 143/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 1.3119e-08 - acc: 1.0000\n",
            "Epoch 143: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 22ms/step - loss: 1.3048e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9980\n",
            "Epoch 144/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 1.4019e-08 - acc: 1.0000\n",
            "Epoch 144: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 3s 29ms/step - loss: 1.3941e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9980\n",
            "Epoch 145/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 1.5321e-08 - acc: 1.0000\n",
            "Epoch 145: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 3s 30ms/step - loss: 1.5321e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9980\n",
            "Epoch 146/200\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 1.8666e-08 - acc: 1.0000\n",
            "Epoch 146: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 23ms/step - loss: 1.8433e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9980\n",
            "Epoch 147/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 1.4841e-08 - acc: 1.0000\n",
            "Epoch 147: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 22ms/step - loss: 1.4841e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9980\n",
            "Epoch 148/200\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 1.8635e-08 - acc: 1.0000\n",
            "Epoch 148: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 22ms/step - loss: 1.8404e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9980\n",
            "Epoch 149/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 1.8421e-08 - acc: 1.0000\n",
            "Epoch 149: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 3s 29ms/step - loss: 1.8421e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9980\n",
            "Epoch 150/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 1.6860e-08 - acc: 1.0000\n",
            "Epoch 150: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 4s 39ms/step - loss: 1.6706e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9980\n",
            "Epoch 151/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 1.2824e-08 - acc: 1.0000\n",
            "Epoch 151: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 3s 33ms/step - loss: 1.2721e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9980\n",
            "Epoch 152/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 1.6126e-08 - acc: 1.0000\n",
            "Epoch 152: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 1.6126e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9980\n",
            "Epoch 153/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 1.7104e-08 - acc: 1.0000\n",
            "Epoch 153: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 22ms/step - loss: 1.6966e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9980\n",
            "Epoch 154/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 1.5440e-08 - acc: 1.0000\n",
            "Epoch 154: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 23ms/step - loss: 1.5321e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9980\n",
            "Epoch 155/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 1.8262e-08 - acc: 1.0000\n",
            "Epoch 155: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 23ms/step - loss: 1.8262e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9980\n",
            "Epoch 156/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 1.5676e-08 - acc: 1.0000\n",
            "Epoch 156: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 3s 26ms/step - loss: 1.5571e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9980\n",
            "Epoch 157/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 1.6656e-08 - acc: 1.0000\n",
            "Epoch 157: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 3s 34ms/step - loss: 1.6963e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9980\n",
            "Epoch 158/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 1.4626e-08 - acc: 1.0000\n",
            "Epoch 158: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 3s 25ms/step - loss: 1.4626e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9980\n",
            "Epoch 159/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 1.6504e-08 - acc: 1.0000\n",
            "Epoch 159: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 22ms/step - loss: 1.6451e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9980\n",
            "Epoch 160/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 1.6569e-08 - acc: 1.0000\n",
            "Epoch 160: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 22ms/step - loss: 1.6470e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9980\n",
            "Epoch 161/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 1.6996e-08 - acc: 1.0000\n",
            "Epoch 161: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 1.6996e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9980\n",
            "Epoch 162/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 1.4607e-08 - acc: 1.0000\n",
            "Epoch 162: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 3s 30ms/step - loss: 1.4753e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9980\n",
            "Epoch 163/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 8.6151e-09 - acc: 1.0000\n",
            "Epoch 163: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 3s 32ms/step - loss: 8.6151e-09 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9980\n",
            "Epoch 164/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 1.9293e-08 - acc: 1.0000\n",
            "Epoch 164: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 23ms/step - loss: 1.9197e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9980\n",
            "Epoch 165/200\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 1.3379e-08 - acc: 1.0000\n",
            "Epoch 165: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 1.3188e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9980\n",
            "Epoch 166/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 1.9962e-08 - acc: 1.0000\n",
            "Epoch 166: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 1.9962e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9980\n",
            "Epoch 167/200\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 1.2529e-08 - acc: 1.0000\n",
            "Epoch 167: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 1.2641e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9980\n",
            "Epoch 168/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 1.2147e-08 - acc: 1.0000\n",
            "Epoch 168: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 3s 29ms/step - loss: 1.2115e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9980\n",
            "Epoch 169/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 1.9433e-08 - acc: 1.0000\n",
            "Epoch 169: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 3s 34ms/step - loss: 1.9292e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9980\n",
            "Epoch 170/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 1.5705e-08 - acc: 1.0000\n",
            "Epoch 170: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 23ms/step - loss: 1.5607e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9980\n",
            "Epoch 171/200\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 1.0894e-08 - acc: 1.0000\n",
            "Epoch 171: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 23ms/step - loss: 1.0804e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9980\n",
            "Epoch 172/200\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 1.5240e-08 - acc: 1.0000\n",
            "Epoch 172: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 1.5063e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9970\n",
            "Epoch 173/200\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 1.6199e-08 - acc: 1.0000\n",
            "Epoch 173: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 23ms/step - loss: 1.6429e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9970\n",
            "Epoch 174/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 2.0550e-08 - acc: 1.0000\n",
            "Epoch 174: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 3s 32ms/step - loss: 2.0550e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9970\n",
            "Epoch 175/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 1.5933e-08 - acc: 1.0000\n",
            "Epoch 175: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 3s 30ms/step - loss: 1.5813e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9970\n",
            "Epoch 176/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 1.2390e-08 - acc: 1.0000\n",
            "Epoch 176: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 23ms/step - loss: 1.2298e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9970\n",
            "Epoch 177/200\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 9.9238e-09 - acc: 1.0000\n",
            "Epoch 177: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 23ms/step - loss: 9.9054e-09 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9970\n",
            "Epoch 178/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 1.5448e-08 - acc: 1.0000\n",
            "Epoch 178: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 22ms/step - loss: 1.5683e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9980\n",
            "Epoch 179/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 1.0995e-08 - acc: 1.0000\n",
            "Epoch 179: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 23ms/step - loss: 1.0995e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9970\n",
            "Epoch 180/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 1.1399e-08 - acc: 1.0000\n",
            "Epoch 180: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 3s 29ms/step - loss: 1.1597e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9970\n",
            "Epoch 181/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 1.0624e-08 - acc: 1.0000\n",
            "Epoch 181: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 3s 32ms/step - loss: 1.0624e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9970\n",
            "Epoch 182/200\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 1.8987e-08 - acc: 1.0000\n",
            "Epoch 182: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 22ms/step - loss: 1.8820e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9970\n",
            "Epoch 183/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 1.2703e-08 - acc: 1.0000\n",
            "Epoch 183: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 23ms/step - loss: 1.2685e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9970\n",
            "Epoch 184/200\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 9.8240e-09 - acc: 1.0000\n",
            "Epoch 184: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 9.8554e-09 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9970\n",
            "Epoch 185/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 1.1070e-08 - acc: 1.0000\n",
            "Epoch 185: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 23ms/step - loss: 1.1070e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9970\n",
            "Epoch 186/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 1.3664e-08 - acc: 1.0000\n",
            "Epoch 186: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 3s 29ms/step - loss: 1.3749e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9970\n",
            "Epoch 187/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 9.6362e-09 - acc: 1.0000\n",
            "Epoch 187: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 4s 36ms/step - loss: 9.6362e-09 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9970\n",
            "Epoch 188/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 1.2056e-08 - acc: 1.0000\n",
            "Epoch 188: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 23ms/step - loss: 1.2131e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9970\n",
            "Epoch 189/200\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 3.0105e-08 - acc: 1.0000\n",
            "Epoch 189: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 2.9796e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9970\n",
            "Epoch 190/200\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 1.2293e-08 - acc: 1.0000\n",
            "Epoch 190: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 1.2206e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9970\n",
            "Epoch 191/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 1.0497e-08 - acc: 1.0000\n",
            "Epoch 191: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 23ms/step - loss: 1.0464e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9970\n",
            "Epoch 192/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 1.5346e-08 - acc: 1.0000\n",
            "Epoch 192: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 3s 33ms/step - loss: 1.5481e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9970\n",
            "Epoch 193/200\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 1.1235e-08 - acc: 1.0000\n",
            "Epoch 193: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 3s 31ms/step - loss: 1.1063e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9970\n",
            "Epoch 194/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 1.2933e-08 - acc: 1.0000\n",
            "Epoch 194: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 1.2857e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9970\n",
            "Epoch 195/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 1.1633e-08 - acc: 1.0000\n",
            "Epoch 195: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 23ms/step - loss: 1.1842e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9970\n",
            "Epoch 196/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 1.6781e-08 - acc: 1.0000\n",
            "Epoch 196: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 1.6781e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9970\n",
            "Epoch 197/200\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 7.8397e-09 - acc: 1.0000\n",
            "Epoch 197: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 9.5906e-09 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9970\n",
            "Epoch 198/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 1.2970e-08 - acc: 1.0000\n",
            "Epoch 198: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 4s 36ms/step - loss: 1.2868e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9970\n",
            "Epoch 199/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 1.0111e-08 - acc: 1.0000\n",
            "Epoch 199: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 3s 27ms/step - loss: 1.0111e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9970\n",
            "Epoch 200/200\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 1.4215e-08 - acc: 1.0000\n",
            "Epoch 200: val_loss did not improve from 0.01310\n",
            "100/100 [==============================] - 2s 23ms/step - loss: 1.3999e-08 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9970\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(train_features, train_labels, epochs= 200, batch_size=20, validation_data = (validation_features, validation_labels), callbacks = [check_point])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 887
        },
        "id": "gB2-x6W-ujBv",
        "outputId": "8db715a3-3dc4-4af4-ddaf-7f9fd394ff36"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs = range(1, len(acc)+1)\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HenHj8Q1vQLe"
      },
      "outputs": [],
      "source": [
        "model.save('cats_and_dogs_small_3.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Oae-pyV49ElT"
      },
      "outputs": [],
      "source": [
        "model = models.Sequential()\n",
        "model.add(conve_base)\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(256, activation = 'relu'))\n",
        "model.add(layers.Dense(1, activation = 'sigmoid'))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here i defines a new model using Keras' sequential API. The model consists of the following layers:\n",
        "1. `conve_base`: This layer is the pre-trained VGG16 model, which is used as a feature extractor and its weights are not trainable.\n",
        "2. `layers.Flatten()`: This layer flattens the output from the VGG16 model into a 1D tensor, preparing it for the dense layers.\n",
        "3. `layers.Dense(256, activation='relu')`: This fully connected dense layer with 256 units applies the ReLU activation function.\n",
        "4. `layers.Dense(1, activation='sigmoid')`: This is the output layer with a single unit using the sigmoid activation function, suitable for binary classification.\n",
        "\n",
        "The resulting model combines the VGG16 base as a feature extractor with additional dense layers for fine-tuning and classification."
      ],
      "metadata": {
        "id": "S-mOpXyLDVw8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8WgZ6bbd9MVF",
        "outputId": "4478e69a-fe8f-4b2e-91ce-52ac6ffdaf43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " vgg16 (Functional)          (None, 4, 4, 512)         14714688  \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 8192)              0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 256)               2097408   \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1)                 257       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 16,812,353\n",
            "Trainable params: 16,812,353\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJ-Z3ons-IuY",
        "outputId": "6f436391-6a2a-4afb-80ed-c34636ac168a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "this is the number of trainable weights before freezing the covolutional base 30\n"
          ]
        }
      ],
      "source": [
        "print('this is the number of trainable weights before freezing the covolutional base', len(model.trainable_weights))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "S4IlxOgSAn6S"
      },
      "outputs": [],
      "source": [
        "conve_base.trainable = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YW2saVPeAzhJ",
        "outputId": "1f95f38b-8eb8-49ad-e3c3-a8fbc01acb66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " this the number of tainable weight after freezing the conve_base 4\n"
          ]
        }
      ],
      "source": [
        "print(' this the number of tainable weight after freezing the conve_base', len(model.trainable_weights))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Wyi0XaioBAxb"
      },
      "outputs": [],
      "source": [
        "model.compile(loss = 'binary_crossentropy', optimizer=optimizers.RMSprop(learning_rate = 2e-5), metrics = ['acc'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vAJzvUihDAmR"
      },
      "outputs": [],
      "source": [
        "history =  model.fit_generator( train_generator, steps_per_epoch= 100, epochs=30, validation_data=validation_generator,validation_steps = 50, callbacks = [check_point] )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here i, first prints the number of trainable weights before freezing the convolutional base, then sets the `conve_base` as non-trainable, and finally prints the number of trainable weights after freezing. The model is then compiled with the specified optimizer, loss function, and metrics. It is subsequently trained using the training data generator with 100 steps per epoch and for 30 epochs, while also using the validation data generator with 50 validation steps. Additionally, it saves the best model based on validation loss using the `ModelCheckpoint` callback."
      ],
      "metadata": {
        "id": "s5sCkbQlFOG5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "SHoX4Sn3EOfC"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs = range(1, len(acc)+1)\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hywNTx_3hV2Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}